# -*- coding: utf-8 -*-
"""LangChain Using HF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1olhn5PvOvcwqjIDmvIpgmOeu3Ft4eM-L
"""

!pip install langchain-huggingface
!pip install huggingface_hub
!pip install transformers
!pip install accelerate
!pip install  bitsandbytes
!pip install langchain

## Environment secret keys
from google.colab import userdata
sec_key=userdata.get("HF_TOKEN")
print(sec_key)

from langchain_huggingface import HuggingFaceEndpoint

from google.colab import userdata
sec_key=userdata.get("HUGGINGFACEHUB")
print(sec_key)

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"]=sec_key

repo_id="mistralai/Mistral-7B-Instruct-v0.3"
llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=128,temperature=0.7,token=sec_key)

llm.invoke("What is Genertaive AI")

from langchain import PromptTemplate, LLMChain

question="Who won the Cricket World Cup in the year 2011?"
template = """Question: {question}
Answer: Let's think step by step."""
prompt = PromptTemplate(template=template, input_variables=["question"])
print(prompt)

llm_chain=LLMChain(llm=llm,prompt=prompt)
print(llm_chain.invoke(question))

!pip install gradio

import gradio as gr

def answer_question(question):
    response = llm_chain.invoke(question)
    return response

# Create the Gradio interface
interface = gr.Interface(
    fn=answer_question,
    inputs=gr.Textbox(label="Enter your question"),
    outputs=gr.Textbox(label="Answer")
)

interface.launch()